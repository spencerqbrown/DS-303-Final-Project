---
title: "DS 303 Final Project Recidivism Classifier"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
df = read.csv("data_cleaned.csv")
```


# Logistic Regression Model
One question that the topic of the data is begging to be asked is if we can predict whether someone released will return to prison within the three year time frame of the data. First, let's see what proportion returned:
```{r}
sum(df$Return.to.Prison) / length(df$Return.to.Prison)
```
So around a third of prisoners returned within three years. 

Let's consider what values are useful for predicting Return.to.Prison. There are some variables we will need to consider altering, combining, or perhaps dropping, like Main.Supervising.District (MSD), which appears to be correlated with Release.Type (i.e., there is often no MSD when a prisoner is put on parole). Similar correlation is present in Target.Population. We will save these for later. Fiscal.Year.Released and Recidivism.Reporting.Year are 2010 and 2013 respectively for all inmates, since they are only labels of the dataset itself, so we will not use them for the model. Additionally, we are predicting whether recidivism will occur, so we will not include variables that confirm it, meaning any new-offense-related variables. These include: Days.to.Return, Recidivism.Type, New.Offense.Classification, New.Offense.Type, and New.Offense.Sub.Type. A non-null value for any of these would, of course, confirm recidivism. We will hold off on excluding or combining the potentially correlated variables, Main.Supervising.District, Release.Type, and Target.Population, for now. We can later perform Chi-squared tests of independence on them. We will tentatively include them in the model. Below we also drop the single row with the value Interstate Compact Parole in Release.Type, as it is the only row with that value.

Before we fit the model, we will remove some rows with very rare values. While it is possible that these values may be good predictors, they are very rare (1-2 instances), so will not be especially useful in predicting the vast majority of cases. They also tend to cause issues when splitting training and testing sets.
```{r}
# drop rows with very rare values (<3)
length(df$Release.Type)
indices_to_remove = vector()
indices_to_remove = append(indices_to_remove, which(df$Release.Type=="Interstate Compact Parole"))
indices_to_remove = append(indices_to_remove, which(df$Age.At.Release==""))
indices_to_remove = append(indices_to_remove, which(df$Race...Ethnicity=="Black -"))
indices_to_remove = append(indices_to_remove, which(df$Race...Ethnicity=="N/A -"))
indices_to_remove = append(indices_to_remove, which(df$Offense.Classification=="Other Misdemeanor"))
indices_to_remove = append(indices_to_remove, which(df$Offense.Classification=="Sexual Predator Community Supervision"))
indices_to_remove = append(indices_to_remove, which(df$Offense.Classification=="Other Felony (Old Code)"))
df = df[-c(indices_to_remove),]
length(df$Release.Type)
```

Now, let's look at a full model using all of our potentially useful variables:
```{r}
# first, train test split using ratio of 6/4
set.seed(20)
train = sample(1:nrow(df),nrow(df)*0.6, replace=FALSE)
test = (-train)

# fit the model
model.fit = glm(Return.to.Prison~Main.Supervising.District+Release.Type+Race...Ethnicity+Age.At.Release+Sex+Offense.Classification+Offense.Subtype, family="binomial", data=df, subset=train)
#summary(model.fit)
```
Uncomment the summary line if necessary. Here it is commented out because the output is very long. Note that there are many many coefficients. This is because every variable is either boolean or categorical, and most of the categorical variables have several categories, resulting in a huge number of coefficients. Now, let's look at our actual predictions.
```{r}
model.prob = predict(model.fit, df[test,],type='response')
head(model.prob)
model.pred = rep(FALSE, nrow(df)-length(test))
model.pred[model.prob >0.5] = TRUE
table(model.pred, df[test,]$Return.to.Prison)
1-mean(model.pred == df[test,]$Return.to.Prison)
```

Not a fantastic accuracy, around 1/3. Next, we will see if we can improve the model using stepwise selection.

Let us the stepAIC from the MASS package to perform both forwards and backwards selection on our full model in an attempt to find a better one. Note that stepAIC uses AIC to make its decision.
```{r}
library(MASS)

model.step = stepAIC(model.fit, trace=FALSE)
coef(model.step)
```

Now, we can see how this model compares to our full model.
```{r}
model.step.prob = predict(model.step, df[test,],type='response')
head(model.step.prob)
model.step.pred = rep(FALSE, nrow(df)-length(test))
model.step.pred[model.prob > 0.5] = TRUE
table(model.step.pred, df[test,]$Return.to.Prison)
1-mean(model.step.pred == df[test,]$Return.to.Prison)
```

So, again, not good. Exactly as not good as the full model. We can see with the following code that the coefficients are actually the same between the two models:
```{r}
sum(coef(model.fit)==coef(model.step)) / length(coef(model.fit))
```
So, 100% matching coefficients. I.e., the stepwise selection found that the full model was the best.

Now let us attempt LDA and QDA models to predict 3 year recidivism.

# LDA and QDA